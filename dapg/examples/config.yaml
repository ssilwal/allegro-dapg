# SLURM config
defaults:
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog
  - override hydra/launcher: submitit_slurm
  # - override hydra/launcher: joblib

    
hydra:
  run:  # don't put outputs in HOME, use SCRATCH instead
    dir: ${env:SCRATCH}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${env:SCRATCH}/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
  launcher:
    #gres: gpu:1     # use this on the nyu greene cluster instead of partition
    timeout_min: 15     
    tasks_per_node: 1
    cpus_per_task: 1
    nodes: 1
    mem_gb: 16     # don't go nuts
    max_num_timeout: 0     # increase if you support requeuing / preemption
    gpus_per_node: null     # don't use this, use gres
    partition: null     # don't use this
    comment: null     # optionally use this to tell others on the cluster what your job is up to
    setup: ['bash setup.sh']
# weights and biases config
wandb: False     # prefer to override this on commandline to avoid cluttering your wandb profile
wbentity: null     # I think this defaults to the `wandb login` user, so no need to override
wbproject: scratch     # set this to appropriate project name
group: default     # I use group names for different sweeps

name: null     # name=1,2,3 is a convenient way to schedule repeats with the same other hyperparameters

cuda: true     # override to manually disable cuda, otherwise will use gpu if available (silently switches to cpu if not)
random_seed: 42
log_freq: 5     # frequency of logging in *seconds*

job_data:
  test_name: 't1'
  env: 'block-v3'
  algorithm: DAPG
  seed: 123
  num_cpu: 5
  save_freq: 25
  eval_rollouts: 25
  exp_notes: 'training policy using DAPG for the block-v0 task, 24 demos'

  # Demonstration data and behavior cloning
  demo_file:  '/home/ss14499/development/hydra/dapg/dapg/demonstrations/oct-block-v0_demos.pickle'
  bc_batch_size: 32
  bc_epochs: 20
  bc_learn_rate: 1e-2

  # RL parameters (all params related to PG, value function, DAPG etc.)
  policy_size: [32, 32]
  vf_batch_size: 64
  vf_epochs: 2
  vf_learn_rate: 1e-3
  rl_step_size: 0.05
  rl_gamma: 0.995
  rl_gae: 0.97
  rl_num_traj: 300
  rl_num_iter: 300
  lam_0: 1e-2
  lam_1: 0.95
